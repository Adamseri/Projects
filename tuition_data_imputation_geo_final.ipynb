{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59ecc6c8",
   "metadata": {},
   "source": [
    " In this first step, we import necessary libraries and define key file paths and varaibles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "555d1d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "DATA_PATH = 'draft_data.csv'\n",
    "SHAPEFILE_PATH = r'tl_2018_us_county.shp'\n",
    "K = 5\n",
    "\n",
    "#Define the tuition-related variables\n",
    "tuition_cols = [\n",
    "    'TU_costt4a', 'TU_npt4_priv', 'TU_npt4_pub', 'TU_pct_pell',\n",
    "    'TU_tuitfte', 'TU_tuition_in', 'TU_tuition_out', 'TU_tuition_prog'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b808bfd",
   "metadata": {},
   "source": [
    "Load the monthly dataset and prepare it for annual aggregation by creating a datetime object and averaging. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1abcf764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in monthly dataset: 226152\n",
      "Missingness per tuition variable (monthly dataset):\n",
      "TU_costt4a         149868\n",
      "TU_npt4_priv       173988\n",
      "TU_npt4_pub        158700\n",
      "TU_pct_pell        141480\n",
      "TU_tuitfte         140988\n",
      "TU_tuition_in      146904\n",
      "TU_tuition_out     147504\n",
      "TU_tuition_prog    182172\n",
      "dtype: int64\n",
      "Rows with all tuition variables NA: 138720\n",
      "Rows with all tuition variables populated: 30588\n",
      "Number of rows in yearly dataset: 18846\n",
      "Rows with all tuition variables NA: 11560\n",
      "Rows with all tuition variables populated: 2549\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_monthly = pd.read_csv(DATA_PATH)\n",
    "\n",
    "df_monthly['month'] = pd.to_datetime(df_monthly['month'], errors='coerce').dt.month\n",
    "df_monthly['year'] = df_monthly['year'].astype(int)\n",
    "df_monthly['Date'] = pd.to_datetime(dict(year=df_monthly['year'], month=df_monthly['month'], day=1))\n",
    "\n",
    "group_cols = ['FIPS', 'year']\n",
    "df_yearly = df_monthly.groupby(group_cols)[tuition_cols].mean().reset_index()\n",
    "\n",
    "\n",
    "\n",
    "#print the length of the original dataframe\n",
    "print(\"Number of rows in monthly dataset:\",len(df_monthly))\n",
    "\n",
    "#Count rows where **all** tuition variables are NA\n",
    "count_all_na_month= df_monthly[tuition_cols].isna().all(axis=1).sum()\n",
    "\n",
    "\n",
    "#Count rows where **all** tuition variables are populated (no NAs)\n",
    "count_all_populated_month = df_monthly[tuition_cols].notna().all(axis=1).sum()\n",
    "\n",
    "#Missingness per tuition variable (monthly dataset)\n",
    "missing_counts = df_monthly[tuition_cols].isna().sum()\n",
    "print(\"Missingness per tuition variable (monthly dataset):\")\n",
    "print(missing_counts)\n",
    "\n",
    "print(\"Rows with all tuition variables NA:\", count_all_na_month)\n",
    "\n",
    "print(\"Rows with all tuition variables populated:\", count_all_populated_month)\n",
    "\n",
    "\n",
    "#print the length of the yearly dataframe\n",
    "print(\"Number of rows in yearly dataset:\",len(df_yearly))\n",
    "\n",
    "#Count rows where **all** tuition variables are NA\n",
    "count_all_na = df_yearly[tuition_cols].isna().all(axis=1).sum()\n",
    "\n",
    "\n",
    "#Count rows where **all** tuition variables are populated (no NAs)\n",
    "count_all_populated = df_yearly[tuition_cols].notna().all(axis=1).sum()\n",
    "\n",
    "\n",
    "print(\"Rows with all tuition variables NA:\", count_all_na)\n",
    "\n",
    "print(\"Rows with all tuition variables populated:\", count_all_populated)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be761fc",
   "metadata": {},
   "source": [
    "Loading shapefile and computing centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fca4994f",
   "metadata": {},
   "outputs": [
    {
     "ename": "DataSourceError",
     "evalue": "tl_2018_us_county.shp: No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mDataSourceError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m gdf = \u001b[43mgpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mSHAPEFILE_PATH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m gdf = gdf.set_crs(epsg=\u001b[32m4269\u001b[39m).to_crs(epsg=\u001b[32m5070\u001b[39m)\n\u001b[32m      3\u001b[39m gdf[\u001b[33m'\u001b[39m\u001b[33mcentroid\u001b[39m\u001b[33m'\u001b[39m] = gdf.geometry.centroid\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aserilevi\\coding\\.venv\\Lib\\site-packages\\geopandas\\io\\file.py:316\u001b[39m, in \u001b[36m_read_file\u001b[39m\u001b[34m(filename, bbox, mask, columns, rows, engine, **kwargs)\u001b[39m\n\u001b[32m    313\u001b[39m             filename = response.read()\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m engine == \u001b[33m\"\u001b[39m\u001b[33mpyogrio\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read_file_pyogrio\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    317\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbbox\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbbox\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrows\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    318\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    320\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m engine == \u001b[33m\"\u001b[39m\u001b[33mfiona\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    321\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m pd.api.types.is_file_like(filename):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aserilevi\\coding\\.venv\\Lib\\site-packages\\geopandas\\io\\file.py:576\u001b[39m, in \u001b[36m_read_file_pyogrio\u001b[39m\u001b[34m(path_or_bytes, bbox, mask, rows, **kwargs)\u001b[39m\n\u001b[32m    567\u001b[39m     warnings.warn(\n\u001b[32m    568\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe \u001b[39m\u001b[33m'\u001b[39m\u001b[33minclude_fields\u001b[39m\u001b[33m'\u001b[39m\u001b[33m and \u001b[39m\u001b[33m'\u001b[39m\u001b[33mignore_fields\u001b[39m\u001b[33m'\u001b[39m\u001b[33m keywords are deprecated, and \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    569\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mwill be removed in a future release. You can use the \u001b[39m\u001b[33m'\u001b[39m\u001b[33mcolumns\u001b[39m\u001b[33m'\u001b[39m\u001b[33m keyword \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    572\u001b[39m         stacklevel=\u001b[32m3\u001b[39m,\n\u001b[32m    573\u001b[39m     )\n\u001b[32m    574\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mcolumns\u001b[39m\u001b[33m\"\u001b[39m] = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33minclude_fields\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m576\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpyogrio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_bytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbbox\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbbox\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aserilevi\\coding\\.venv\\Lib\\site-packages\\pyogrio\\geopandas.py:275\u001b[39m, in \u001b[36mread_dataframe\u001b[39m\u001b[34m(path_or_buffer, layer, encoding, columns, read_geometry, force_2d, skip_features, max_features, where, bbox, mask, fids, sql, sql_dialect, fid_as_index, use_arrow, on_invalid, arrow_to_pandas_kwargs, **kwargs)\u001b[39m\n\u001b[32m    270\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m use_arrow:\n\u001b[32m    271\u001b[39m     \u001b[38;5;66;03m# For arrow, datetimes are read as is.\u001b[39;00m\n\u001b[32m    272\u001b[39m     \u001b[38;5;66;03m# For numpy IO, datetimes are read as string values to preserve timezone info\u001b[39;00m\n\u001b[32m    273\u001b[39m     \u001b[38;5;66;03m# as numpy does not directly support timezones.\u001b[39;00m\n\u001b[32m    274\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mdatetime_as_string\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m275\u001b[39m result = \u001b[43mread_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    276\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    277\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    278\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    279\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    280\u001b[39m \u001b[43m    \u001b[49m\u001b[43mread_geometry\u001b[49m\u001b[43m=\u001b[49m\u001b[43mread_geometry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    281\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforce_2d\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgdal_force_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    282\u001b[39m \u001b[43m    \u001b[49m\u001b[43mskip_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskip_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    283\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    284\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    285\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbbox\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbbox\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    286\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    287\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    288\u001b[39m \u001b[43m    \u001b[49m\u001b[43msql\u001b[49m\u001b[43m=\u001b[49m\u001b[43msql\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    289\u001b[39m \u001b[43m    \u001b[49m\u001b[43msql_dialect\u001b[49m\u001b[43m=\u001b[49m\u001b[43msql_dialect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    290\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_fids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfid_as_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    291\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    292\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    294\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m use_arrow:\n\u001b[32m    295\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyarrow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpa\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aserilevi\\coding\\.venv\\Lib\\site-packages\\pyogrio\\raw.py:198\u001b[39m, in \u001b[36mread\u001b[39m\u001b[34m(path_or_buffer, layer, encoding, columns, read_geometry, force_2d, skip_features, max_features, where, bbox, mask, fids, sql, sql_dialect, return_fids, datetime_as_string, **kwargs)\u001b[39m\n\u001b[32m     59\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Read OGR data source into numpy arrays.\u001b[39;00m\n\u001b[32m     60\u001b[39m \n\u001b[32m     61\u001b[39m \u001b[33;03mIMPORTANT: non-linear geometry types (e.g., MultiSurface) are converted\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    194\u001b[39m \n\u001b[32m    195\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    196\u001b[39m dataset_kwargs = _preprocess_options_key_value(kwargs) \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[32m--> \u001b[39m\u001b[32m198\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mogr_read\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[43m    \u001b[49m\u001b[43mget_vsi_path_or_buffer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_buffer\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    201\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    203\u001b[39m \u001b[43m    \u001b[49m\u001b[43mread_geometry\u001b[49m\u001b[43m=\u001b[49m\u001b[43mread_geometry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforce_2d\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    205\u001b[39m \u001b[43m    \u001b[49m\u001b[43mskip_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskip_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    206\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_features\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    207\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    208\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbbox\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbbox\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    209\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_mask_to_wkb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    210\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    211\u001b[39m \u001b[43m    \u001b[49m\u001b[43msql\u001b[49m\u001b[43m=\u001b[49m\u001b[43msql\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    212\u001b[39m \u001b[43m    \u001b[49m\u001b[43msql_dialect\u001b[49m\u001b[43m=\u001b[49m\u001b[43msql_dialect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    213\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_fids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_fids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    214\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataset_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataset_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    215\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdatetime_as_string\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdatetime_as_string\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    216\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpyogrio\\\\_io.pyx:1293\u001b[39m, in \u001b[36mpyogrio._io.ogr_read\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpyogrio\\\\_io.pyx:232\u001b[39m, in \u001b[36mpyogrio._io.ogr_open\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mDataSourceError\u001b[39m: tl_2018_us_county.shp: No such file or directory"
     ]
    }
   ],
   "source": [
    "\n",
    "gdf = gpd.read_file(SHAPEFILE_PATH)\n",
    "gdf = gdf.set_crs(epsg=4269).to_crs(epsg=5070)\n",
    "gdf['centroid'] = gdf.geometry.centroid\n",
    "gdf['centroid_x'] = gdf.centroid.x\n",
    "gdf['centroid_y'] = gdf.centroid.y\n",
    "gdf['GEOID'] = gdf['GEOID'].astype(str)\n",
    "\n",
    "df_yearly['FIPS'] = df_yearly['FIPS'].astype(str).str.zfill(5)\n",
    "df_yearly = df_yearly.merge(\n",
    "    gdf[['GEOID', 'centroid_x', 'centroid_y']],\n",
    "    left_on='FIPS',\n",
    "    right_on='GEOID',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "df_yearly[['FIPS', 'centroid_x', 'centroid_y']].head()\n",
    "\n",
    "df_yearly_kfold=df_yearly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2909f8",
   "metadata": {},
   "source": [
    "Defining functions for imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b3fe15",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def prepare_features(df, target_col):\n",
    "    mask = (\n",
    "        df[target_col].notna() &\n",
    "        df['centroid_x'].notna() &\n",
    "        df['centroid_y'].notna()\n",
    "    )\n",
    "    features = ['centroid_x', 'centroid_y']\n",
    "    X = df.loc[mask, features].copy()\n",
    "    X_raw = X.values  \n",
    "    return X_raw, df.loc[mask].index, None  \n",
    "\n",
    "\n",
    "def knn_geo_impute(df, col, k=5):\n",
    "    df_copy = df.copy()\n",
    "    X_train_scaled, train_indices, scaler = prepare_features(df_copy, col)\n",
    "    y_train = df_copy.loc[train_indices, col].values\n",
    "    assert not np.isnan(y_train).any(), f\"Training data for {col} contains missing values!\"\n",
    "\n",
    "    if len(X_train_scaled) == 0:\n",
    "        print(f\" No training data available for {col}, skipping imputation.\")\n",
    "        return df_copy, 0\n",
    "\n",
    "    nbrs = NearestNeighbors(n_neighbors=min(k, len(X_train_scaled))).fit(X_train_scaled)\n",
    "\n",
    "    mask_test = (\n",
    "        df_copy[col].isna() &\n",
    "        df_copy['centroid_x'].notna() &\n",
    "        df_copy['centroid_y'].notna()\n",
    "    )\n",
    "    X_test = df_copy.loc[mask_test, ['centroid_x', 'centroid_y']].copy()\n",
    "    if X_test.empty:\n",
    "        return df_copy, 0\n",
    "\n",
    "    X_test_scaled = X_test.values\n",
    "\n",
    "    could_not_impute = 0\n",
    "\n",
    "    for idx, x in tqdm(zip(X_test.index, X_test_scaled), total=X_test.shape[0], desc=f\"Imputing {col}\", leave=False):\n",
    "        distances, neighbor_ids = nbrs.kneighbors([x], n_neighbors=k)\n",
    "        neighbor_vals = y_train[neighbor_ids[0]]\n",
    "        valid_vals = neighbor_vals[~np.isnan(neighbor_vals)]\n",
    "        if len(valid_vals) > 0:\n",
    "            df_copy.at[idx, col] = valid_vals.mean()\n",
    "        else:\n",
    "            could_not_impute += 1\n",
    "\n",
    "    return df_copy, could_not_impute\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf4a841",
   "metadata": {},
   "source": [
    "Apply KNN imputation to each tuition column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f771ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting geo-only imputation at county-year level...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing tuition variables: 100%|██████████| 8/8 [00:40<00:00,  5.05s/it]\n"
     ]
    }
   ],
   "source": [
    "imputed_flags = pd.DataFrame(index=df_yearly.index)\n",
    "print(\"Starting geo-only imputation at county-year level...\")\n",
    "\n",
    "for col in tqdm(tuition_cols, desc=\"Processing tuition variables\"):\n",
    "    original_known = df_yearly[col].notna()\n",
    "    train_mask = (\n",
    "        original_known & \n",
    "        df_yearly['centroid_x'].notna() & \n",
    "        df_yearly['centroid_y'].notna()\n",
    "    )\n",
    "    if train_mask.sum() < K:\n",
    "        print(f\" Skipping {col}: not enough known data to find {K} neighbors.\")\n",
    "        imputed_flags[col] = False\n",
    "        continue\n",
    "\n",
    "    df_yearly, could_not_impute = knn_geo_impute(df_yearly, col, k=K)\n",
    "    now_known = df_yearly[col].notna()\n",
    "    was_imputed = now_known & (~original_known)\n",
    "    imputed_flags[col] = was_imputed\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64864ca6",
   "metadata": {},
   "source": [
    "Merge imputed yearly data back into the monthly dataset and save both versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f147320d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_monthly['FIPS'] = df_monthly['FIPS'].astype(str).str.zfill(5)\n",
    "df_imputed = df_monthly.merge(\n",
    "    df_yearly[group_cols + tuition_cols],\n",
    "    on=['FIPS', 'year'],\n",
    "    suffixes=('', '_imputed')\n",
    ")\n",
    "\n",
    "for col in tuition_cols:\n",
    "    df_imputed[col] = df_imputed[col].fillna(df_imputed[f'{col}_imputed'])\n",
    "    df_imputed.drop(columns=[f'{col}_imputed'], inplace=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "54fdec07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export imputed data, one with original year data that was used in the imputation, and another with the deaggregated imputated dataset\n",
    "df_yearly.to_csv(\"imputed_data_geo_county_year.csv\", index=False)\n",
    "df_imputed.to_csv(\"imputed_data_geo_county_month.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2729aa",
   "metadata": {},
   "source": [
    "After imputation, we display how many values remain missing for each tuition variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ec38f9f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "All imputation complete and monthly data reconstructed.\n",
      "18846 county-year rows | 226152 county-month rows\n",
      "\n",
      "Final Missing Value Report:\n",
      "                 Missing Count\n",
      "TU_costt4a                  12\n",
      "TU_npt4_priv                12\n",
      "TU_npt4_pub                 12\n",
      "TU_pct_pell                 12\n",
      "TU_tuitfte                  12\n",
      "TU_tuition_in               12\n",
      "TU_tuition_out              12\n",
      "TU_tuition_prog             12\n"
     ]
    }
   ],
   "source": [
    "#summary and remaining missing values after imputation\n",
    "print(\"\\nAll imputation complete and monthly data reconstructed.\")\n",
    "print(f\"{df_yearly.shape[0]} county-year rows | {df_imputed.shape[0]} county-month rows\")\n",
    "\n",
    "print(\"\\nFinal Missing Value Report:\")\n",
    "missing_summary = df_yearly[tuition_cols].isna().sum().to_frame(name='Missing Count')\n",
    "print(missing_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0842810",
   "metadata": {},
   "source": [
    "In this cell, we summarize imputation coverage across states and years. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9914c7be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Number of Counties Imputed by State and Year:\n",
      "             State  year  NumCountiesImputed\n",
      "258          Texas  2023                 254\n",
      "60         Georgia  2023                 159\n",
      "276       Virginia  2023                 132\n",
      "102       Kentucky  2023                 120\n",
      "150       Missouri  2023                 115\n",
      "..             ...   ...                 ...\n",
      "121  Massachusetts  2018                   5\n",
      "61          Hawaii  2018                   3\n",
      "169  New Hampshire  2018                   3\n",
      "229   Rhode Island  2018                   3\n",
      "43        Delaware  2018                   1\n",
      "\n",
      "[301 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "#inspecting count of imputed values per state\n",
    "state_fips_map = {\n",
    "    '01': 'Alabama', '02': 'Alaska', '04': 'Arizona', '05': 'Arkansas', '06': 'California',\n",
    "    '08': 'Colorado', '09': 'Connecticut', '10': 'Delaware', '11': 'D.C.', '12': 'Florida',\n",
    "    '13': 'Georgia', '15': 'Hawaii', '16': 'Idaho', '17': 'Illinois', '18': 'Indiana',\n",
    "    '19': 'Iowa', '20': 'Kansas', '21': 'Kentucky', '22': 'Louisiana', '23': 'Maine',\n",
    "    '24': 'Maryland', '25': 'Massachusetts', '26': 'Michigan', '27': 'Minnesota',\n",
    "    '28': 'Mississippi', '29': 'Missouri', '30': 'Montana', '31': 'Nebraska', '32': 'Nevada',\n",
    "    '33': 'New Hampshire', '34': 'New Jersey', '35': 'New Mexico', '36': 'New York',\n",
    "    '37': 'North Carolina', '38': 'North Dakota', '39': 'Ohio', '40': 'Oklahoma',\n",
    "    '41': 'Oregon', '42': 'Pennsylvania', '44': 'Rhode Island', '45': 'South Carolina',\n",
    "    '46': 'South Dakota', '47': 'Tennessee', '48': 'Texas', '49': 'Utah', '50': 'Vermont',\n",
    "    '51': 'Virginia', '53': 'Washington', '54': 'West Virginia', '55': 'Wisconsin',\n",
    "    '56': 'Wyoming'\n",
    "}\n",
    "df_yearly['STATEFP'] = df_yearly['FIPS'].str[:2]\n",
    "df_yearly['State'] = df_yearly['STATEFP'].map(state_fips_map)\n",
    "df_yearly['was_any_tuition_imputed'] = imputed_flags[tuition_cols].any(axis=1)\n",
    "\n",
    "impute_counts = (\n",
    "    df_yearly[df_yearly['was_any_tuition_imputed']]\n",
    "    .groupby(['State', 'year'])['FIPS']\n",
    "    .nunique()\n",
    "    .reset_index(name='NumCountiesImputed')\n",
    "    .sort_values(['year', 'NumCountiesImputed'], ascending=[False, False])\n",
    ")\n",
    "\n",
    "print(\"\\n Number of Counties Imputed by State and Year:\")\n",
    "print(impute_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7e7d9990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running cross-validation for K = 5\n",
      "TU_costt4a — RMSE: 8256.277\n",
      "TU_npt4_priv — RMSE: 4749.884\n",
      "TU_npt4_pub — RMSE: 3055.235\n",
      "TU_pct_pell — RMSE: 0.120\n",
      "TU_tuitfte — RMSE: 9835.115\n",
      "TU_tuition_in — RMSE: 6907.523\n",
      "TU_tuition_out — RMSE: 6681.259\n",
      "TU_tuition_prog — RMSE: 5059.953\n",
      "\n",
      " Cross-validated RMSEs with Tuition Mean and Std Dev (K = 5):\n",
      "       Variable    K = 5      Mean    StdDev\n",
      "     TU_costt4a 8256.277 25141.946 15460.875\n",
      "   TU_npt4_priv 4749.884 19038.597  7222.179\n",
      "    TU_npt4_pub 3055.235  9812.648  5027.152\n",
      "    TU_pct_pell    0.120     0.394     0.181\n",
      "     TU_tuitfte 9835.115  8324.475 12274.252\n",
      "  TU_tuition_in 6907.523 13839.635 13590.106\n",
      " TU_tuition_out 6681.259 17775.735 12590.771\n",
      "TU_tuition_prog 5059.953 14598.132  8601.613\n"
     ]
    }
   ],
   "source": [
    "# Load original yearly data (before imputation)\n",
    "#I saved df_yearly_kfold after the step where I merged the centroid coordinates in the very beginning, below I'm just resaving df_yearly to be the df_yearly that was saved at the step after merging the centroids\n",
    "df_yearly=df_yearly_kfold\n",
    "# Tuition and columns\n",
    "tuition_cols = [\n",
    "    'TU_costt4a', 'TU_npt4_priv', 'TU_npt4_pub', 'TU_pct_pell',\n",
    "    'TU_tuitfte', 'TU_tuition_in', 'TU_tuition_out', 'TU_tuition_prog'\n",
    "]\n",
    "\n",
    "# Cross-validation settings\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "K = 5  # Only K=5\n",
    "results_list = []\n",
    "\n",
    "print(f\"\\nRunning cross-validation for K = {K}\")\n",
    "for col in tuition_cols:\n",
    "    valid_mask = (\n",
    "        df_yearly[col].notna() &\n",
    "        df_yearly['centroid_x'].notna() &\n",
    "        df_yearly['centroid_y'].notna() \n",
    "    )\n",
    "    df_valid = df_yearly.loc[valid_mask].reset_index(drop=True)\n",
    "    if len(df_valid) < K:\n",
    "        print(f\" Skipping {col}: insufficient data for K = {K}\")\n",
    "        continue\n",
    "\n",
    "    errors = []\n",
    "    for train_idx, test_idx in kf.split(df_valid):\n",
    "        train_df = df_valid.loc[train_idx]\n",
    "        test_df = df_valid.loc[test_idx]\n",
    "\n",
    "        train_X = train_df[['centroid_x', 'centroid_y']].values\n",
    "        train_y = train_df[col].values\n",
    "\n",
    "        test_preds = []\n",
    "        test_true = []\n",
    "\n",
    "        nbrs = NearestNeighbors(n_neighbors=min(K, len(train_X))).fit(train_X)\n",
    "\n",
    "        for _, row in test_df.iterrows():\n",
    "            x_test = row[['centroid_x', 'centroid_y']].values\n",
    "            x_test = [x_test]  # keep as 2D array\n",
    "            distances, neighbor_ids = nbrs.kneighbors(x_test, n_neighbors=K)\n",
    "            neighbor_vals = train_y[neighbor_ids[0]]\n",
    "            valid_vals = neighbor_vals[~np.isnan(neighbor_vals)]\n",
    "            if len(valid_vals) > 0:\n",
    "                pred = valid_vals.mean()\n",
    "                test_preds.append(pred)\n",
    "                test_true.append(row[col])\n",
    "\n",
    "        if test_preds:\n",
    "            mse = mean_squared_error(test_true, test_preds)\n",
    "            rmse = np.sqrt(mse)\n",
    "            errors.append(rmse)\n",
    "\n",
    "    if errors:\n",
    "        avg_rmse = np.mean(errors)\n",
    "        results_list.append({'K': f\"K = {K}\", 'Variable': col, 'RMSE': avg_rmse})\n",
    "        print(f\"{col} — RMSE: {avg_rmse:.3f}\")\n",
    "    else:\n",
    "        print(f\"{col} — no valid folds to compute RMSE\")\n",
    "\n",
    "# Convert results to a DataFrame\n",
    "results_df = pd.DataFrame(results_list)\n",
    "\n",
    "# Compute mean and std dev for each tuition column (before imputation)\n",
    "summary_stats = df_yearly[tuition_cols].agg(['mean', 'std']).T.reset_index()\n",
    "summary_stats.columns = ['Variable', 'Mean', 'StdDev']\n",
    "\n",
    "# Pivot RMSE results to get RMSE column for K=5\n",
    "rmse_table = (\n",
    "    results_df\n",
    "    .pivot(index='Variable', columns='K', values='RMSE')\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Merge RMSE with summary stats\n",
    "final_table = pd.merge(rmse_table, summary_stats, on='Variable', how='left')\n",
    "\n",
    "# Round for display\n",
    "final_table = final_table.round(3)\n",
    "\n",
    "# Display the final table\n",
    "print(\"\\n Cross-validated RMSEs with Tuition Mean and Std Dev (K = 5):\")\n",
    "print(final_table.to_string(index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
